---
title: 'Research that is not reproducible is like pollution: We should treat it as such'
author: 'Chris '
date: '2019-03-09'
slug: qrps-are-like-pollution-we-should-treat-them-as-such
categories:
 - open science
tags:
 - Open science
image:
 caption: ''
 focal_point: ''
bibliography: [one.bib]
link-citations: TRUE
bilbio-style: "apalike"
---



<p><em>When a measure becomes a target, it ceases to be a good measure</em> - Goodhart’s law</p>
<p>I was recently interviewed by Vinay Patel and Joe Meyer of Louisiana Tech University (shout out to the Bulldogs!) on the topic of open science in IO psychology. In their article titled <a href="http://my.siop.org/Publications/TIP/562/ArtMID/18540/ArticleID/711/We-Want-Open-Science-in-I-O-Do-We">“We want open science in I-O!…Do we?”</a> Vinay and Joe gave a broad overview of what open science is and why it particularly relevant for IO psychology.</p>
<p>As Vinay and Joe allude to, science seems to be going through some growing pains. For instance, the <a href="https://www.bbc.com/news/science-environment-39054778">BBC</a> reported on a survey by <em>Nature</em>, which found that over 70% of scientists have tried and failed to reproduce another scientist’s findings. <a href="https://slate.com/technology/2016/04/biomedicine-facing-a-worse-replication-crisis-than-the-one-plaguing-psychology.html">Slate</a> reported on a series of studies suggesting that when industry scientists tried to replicate effects from the published literature, they quite often failed to do so. One particular project (reproducibility project in cancer biology) selected 50 influential studies for replication. Unfortunately, the results of only 18 studies will be revealed. Why? Because <a href="http://www.sciencemag.org/news/2018/07/plan-replicate-50-high-impact-cancer-papers-shrinks-just-18">a lack of methodological detail made it difficult to reproduce the original findings</a>. Such detail is often lost in the publication process because of concerns that seem less relevant for the 21st century, such as journal space.</p>
<p>In my segment, I focused on how the reproducibility crisis might be like fighting pollution. I think the problem of reproducibilty is wide-spread throughout science…even in I/O psych and the remainder of the organizational sciences.</p>
<p>Keeping the problem of reproducibility in mind, I think we can learn something from viewing a certain cause of irreproducibility - questionable research practices - which are surprisingly common in <a href="https://www.psychologicalscience.org/news/releases/questionable-research-practices-surprisingly-common.html">psychological science</a>, in a way akin to how we view pollution as a cause of climate change. Most of us would agree that pollution is undesirable (e.g., producing acid rain, smog, climate change, etc.). Pollution can be linked to a number of things that we do everyday, the most obvious of which is driving a car. Since the vast majority of us would have to take actions that are undesirable (e.g., drive our cars less often, more efficiently, etc.), collectively compelling people to change their behavior in order to reduce pollution is going to have some difficulties. In other words, pollution is a collective action problem: it is easy for people to take actions that harm the environment - such as driving cars - because the private benefits of doing so outweigh the private costs. Unfortunately, the social costs do not typically factor into our decision to buy a car.</p>
<p>The same might be said of scholarship writ-large and of organizational scholarship particularly: publishing scholars can take steps (e.g., engage in questionable research practices) that pollute the scientific literature to their own benefit (e.g., tenure, promotion, etc.)…and to the detriment of the field, practice, and society at large <span class="citation">(Honig et al. <a href="#ref-honigReflectionsScientificMisconduct2018">2018</a>)</span>. Scholars, should not be viewed as the villains, however. Rather, the system within which they operate deserves criticism. Scholars are merely responding to incentives to publish in high-impact outlets furnished by the system (e.g., pursuing publication in outlets with high impact factors)…or otherwise perish <span class="citation">(Honig et al. <a href="#ref-honigReflectionsScientificMisconduct2018">2018</a>)</span>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> QRPs, which have been defined as “design, analytic, or reporting practices that have been questioned because of the potential for the practice to be employed with the purpose of presenting biased evidence in favor of an assertion” <span class="citation">(G. C. Banks, Rogelberg, et al. <a href="#ref-banksEditorialEvidenceQuestionable2016">2016</a>)</span>, are prevalent in applied psychology. Examples include selectively reporting results that are statistically significant, p-hacking, and adding and removing cases and variables to make results favorable to a particular point of view. Research by Banks et al. suggest that upward of 91% of studies contain evidence of questionable research practices. Scholars noted that QRPs are required to publish in top-tier outlets <span class="citation">(G. C. Banks, O’Boyle, et al. <a href="#ref-banksQuestionsQuestionableResearch2016">2016</a>)</span>.</p>
<p>To resolve the collective action problem of pollution control, a number of policies have been proposed and enacted, such as incentivizing the purchase of more energy efficient or use of clean sources of energy (e.g., solar, wind, etc.)<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> and imposing taxes on carbon. The former rewards environmentally conscious behavior while the latter punishes undesired behavior by helping consumers see the social cost imposed by their decisions. Such an approach is preferred by economists writ-large, who have suggested that any revenues generated by such carbon taxes can be refunded via a flat, universal dividend <span class="citation">(see Exchange <a href="#ref-freeexchangeBraveNewDeal2019">2019</a>)</span></p>
<p>What if we viewed QRPs such as p-hacking <span class="citation">(Wicherts et al. <a href="#ref-wichertsDegreesFreedomPlanning2016">2016</a>)</span> and failing to report all conducted analyses <span class="citation">(Crede and Harms <a href="#ref-credeQuestionableResearchPractices2019">2019</a>)</span> as a collective action problem? Indeed, this has been discussed <a href="https://www.chronicle.com/article/How-to-Fix-Psychology-s/233857">elsewhere</a>). What would be analogous to the incentivizing robust <span class="citation">(see Grand et al. <a href="#ref-grandSystemsBasedApproachFostering2018">2018</a>)</span> or open <span class="citation">(see Nosek et al. <a href="#ref-nosekPromotingOpenResearch2015">2015</a>)</span> research practices to reduce the prevalence of QRPs? Simple ideas include allocating greater weight in promotion, tenure, and in the publication process for scholarship that is open and reproducible <span class="citation">(Chambers <a href="#ref-chambersSevenDeadlySins2017">2017</a>)</span>. This can occur within departments, colleges, and journals. One beneficial by-product would most certainly involve greater transfer over to practice (and back from practice). The models that academics test can be ‘test-driven’, per se, over in practice (and vice versa). Another idea involves tying grants and publishing to robust practices. For instance, <span class="citation">Chambers (<a href="#ref-chambersSevenDeadlySins2017">2017</a>)</span> noted that journals using a two-stage review process whereby methods are critiqued prior to data collection can grant a conditional publication (i.e., publication will occur if data are gathered as agreed to in the review process). An example journal that does this in IO psych is the <em>Journal of Business and Psychology</em>. Such a practice will drastically cut down on QRPs in the reporting phase of a project (e.g., dropping results that don’t align with predictions). Additionally, and most importantly, funding (e.g., Small Grants offered by the Society for Industrial &amp; Organizational Psychology) can prioritize studies that are pre-registered and will be published. This helps to efficiently and effectively allocate resources to those studies that are conducted in a truly scientific manner. Conversely, studies that do not conform to robust or open methodology should be penalized heavily in both academic and practitioner circles. Over time, things will improve.</p>
<p>What do you think?</p>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-banksQuestionsQuestionableResearch2016">
<p>Banks, George C., Ernest H. O’Boyle, Jeffrey M. Pollack, Charles D. White, John H. Batchelor, Christopher E. Whelpley, Kristie A. Abston, Andrew A. Bennett, and Cheryl L. Adkins. 2016. “Questions About Questionable Research Practices in the Field of Management: A Guest Commentary.” <em>Journal of Management</em> 42 (1): 5–20. <a href="https://doi.org/10.1177/0149206315619011" class="uri">https://doi.org/10.1177/0149206315619011</a>.</p>
</div>
<div id="ref-banksEditorialEvidenceQuestionable2016">
<p>Banks, George C., Steven G. Rogelberg, Haley M. Woznyj, Ronald S. Landis, and Deborah E. Rupp. 2016. “Editorial: Evidence on Questionable Research Practices: The Good, the Bad, and the Ugly.” <em>Journal of Business and Psychology</em> 31 (3): 323–38. <a href="https://doi.org/10.1007/s10869-016-9456-7" class="uri">https://doi.org/10.1007/s10869-016-9456-7</a>.</p>
</div>
<div id="ref-brembsPrestigiousScienceJournals2018">
<p>Brembs, Björn. 2018. “Prestigious Science Journals Struggle to Reach Even Average Reliability.” <em>Frontiers in Human Neuroscience</em> 12. <a href="https://doi.org/10.3389/fnhum.2018.00037" class="uri">https://doi.org/10.3389/fnhum.2018.00037</a>.</p>
</div>
<div id="ref-brembsDeepImpactUnintended2013">
<p>Brembs, Björn, Katherine Button, and Marcus Munaf‘o. 2013. “Deep Impact: Unintended Consequences of Journal Rank.” <em>Frontiers in Human Neuroscience</em> 7. <a href="https://doi.org/10.3389/fnhum.2013.00291" class="uri">https://doi.org/10.3389/fnhum.2013.00291</a>.</p>
</div>
<div id="ref-chambersSevenDeadlySins2017">
<p>Chambers, Chris. 2017. <em>The Seven Deadly Sins of Psychology a Manifesto for Reforming the Culture of Scientific Practice</em>.</p>
</div>
<div id="ref-credeQuestionableResearchPractices2019">
<p>Crede, Marcus, and Peter Harms. 2019. “Questionable Research Practices When Using Confirmatory Factor Analysis.” <em>Journal of Managerial Psychology</em> 34 (1): 18–30. <a href="https://doi.org/10.1108/JMP-06-2018-0272" class="uri">https://doi.org/10.1108/JMP-06-2018-0272</a>.</p>
</div>
<div id="ref-freeexchangeBraveNewDeal2019">
<p>Exchange, Free. 2019. “Brave New Deal.” <em>The Economist</em>, February, 67.</p>
</div>
<div id="ref-grandSystemsBasedApproachFostering2018">
<p>Grand, James A., Steven G. Rogelberg, Tammy D. Allen, Ronald S. Landis, Douglas H. Reynolds, John C. Scott, Scott Tonidandel, and Donald M. Truxillo. 2018. “A Systems-Based Approach to Fostering Robust Science in Industrial-Organizational Psychology.” <em>Industrial and Organizational Psychology</em> 11 (01): 4–42. <a href="https://doi.org/10.1017/iop.2017.55" class="uri">https://doi.org/10.1017/iop.2017.55</a>.</p>
</div>
<div id="ref-honigReflectionsScientificMisconduct2018">
<p>Honig, Benson, Joseph Lampel, Joel A. C. Baum, Mary Ann Glynn, Runtian Jing, Michael Lounsbury, Elke Schüßler, et al. 2018. “Reflections on Scientific Misconduct in Management: Unfortunate Incidents or a Normative Crisis?” <em>Academy of Management Perspectives</em> 32 (4): 412–42. <a href="https://doi.org/10.5465/amp.2015.0167" class="uri">https://doi.org/10.5465/amp.2015.0167</a>.</p>
</div>
<div id="ref-nosekPromotingOpenResearch2015">
<p>Nosek, B. A., G. Alter, G. C. Banks, D. Borsboom, S. D. Bowman, S. J. Breckler, S. Buck, et al. 2015. “Promoting an Open Research Culture.” <em>Science</em> 348 (6242): 1422–5. <a href="https://doi.org/10.1126/science.aab2374" class="uri">https://doi.org/10.1126/science.aab2374</a>.</p>
</div>
<div id="ref-wichertsDegreesFreedomPlanning2016">
<p>Wicherts, Jelte M., Coosje L. S. Veldkamp, Hilde E. M. Augusteijn, Marjan Bakker, Robbie C. M. van Aert, and Marcel A. L. M. van Assen. 2016. “Degrees of Freedom in Planning, Running, Analyzing, and Reporting Psychological Studies: A Checklist to Avoid P-Hacking.” <em>Frontiers in Psychology</em> 7 (November). <a href="https://doi.org/10.3389/fpsyg.2016.01832" class="uri">https://doi.org/10.3389/fpsyg.2016.01832</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Ironically, like the quote used to open this post, impact factors do not seem to correlate with high quality contributions. They do, however, predict retractions <span class="citation">(Brembs, Button, and Munaf‘o <a href="#ref-brembsDeepImpactUnintended2013">2013</a>)</span> and are negatively correlated with replicability <span class="citation">(Brembs <a href="#ref-brembsPrestigiousScienceJournals2018">2018</a>)</span>.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>On a personal note, I took advantage of this in buying myself a new car (a Honda Clarity).<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
