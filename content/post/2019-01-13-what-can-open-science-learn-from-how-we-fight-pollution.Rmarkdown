---
title: What can open science learn from how we fight pollution?
author: Chris
date: '2019-01-13'
slug: what-can-open-science-learn-from-how-we-fight-pollution
categories:
  - open science
tags:
  - Open science
image:
  caption: ''
  focal_point: ''
bibliography: [one.bib]
link-citations: TRUE
bilbio-style: "apalike"
---
*When a measure becomes a target, it ceases to be a good measure* - Goodhart's law

I was recently interviewed by Vinay Patel and Joe Meyer of Louisiana Tech University (shout out to the Bulldogs!) on the topic of open science in IO psychology. In their article titled ["We want open science in I-O!...Do we?"](http://my.siop.org/Publications/TIP/562/ArtMID/18540/ArticleID/711/We-Want-Open-Science-in-I-O-Do-We) Vinay and Joe gave a broad overview of what open science is and why it particularly relevant for IO psychology. 

As Vinay and Joe allude to, science seems to be going through some growing pains. For instance, the [BBC](https://www.bbc.com/news/science-environment-39054778) reported on a survey by *Nature*, which found that over 70% of scientists have tried and failed to reproduce another scientist’s findings. [Slate](https://slate.com/technology/2016/04/biomedicine-facing-a-worse-replication-crisis-than-the-one-plaguing-psychology.html) reported on a series of studies suggesting that when industry scientists tried to replicate effects from the published literature, they quite often failed to do so. A particularly troubling project (reproducibility project in cancer biology) selected 50 influential studies for replication. Unfortunately, the results of only 18 studies will be revealed. Why? Because [a lack of methodological detail made it difficult to reproduce the original findings](http://www.sciencemag.org/news/2018/07/plan-replicate-50-high-impact-cancer-papers-shrinks-just-18). Such detail is often lost in the publication process because of concerns that seem less relevant for the 21st century, such as journal space. 

Keeping the problem of reproducibility in mind, I think we can learn something from viewing a certain class of frequently-discussed problems - questionable research practices - which are surprisingly common in [psychological science](https://www.psychologicalscience.org/news/releases/questionable-research-practices-surprisingly-common.html), in a way akin to how we view pollution as a cause of climate change. Most of us would agree that pollution is undesirable (e.g., producing acid rain, smog, climate change, etc.). Pollution can be linked to a number of things that we do everyday, the most obvious of which is driving a car. Since the vast majority of us would have to take actions that are undesirable (e.g., drive our cars less often, more efficiently, etc.), collectively compelling people to change their behavior in order to reduce pollution is going to have some difficulties. In other words, pollution is a collective action problem: it is easy for people to take actions that harm the environment - such as driving cars - because the private benefits of doing so outweigh the private costs. Unfortunately, the social costs do not typically factor into our decision to buy a car.

To resolve this collective action problem, a number of policies have been proposed and enacted, such as incentivizing the purchase of more energy efficient or use of clean sources of energy (e.g., solar, wind, etc.) [^1] and imposing taxes on carbon. The former rewards environmentally conscious behavior while the latter punishes undesired behavor by helping consumers see the social cost imposed by their decisions. Such an approach is preferred by economists writ-large, who have suggested that any revenues generated by such carbon taxes can be refunded via a flat, universal dividend [see @freeexchangeBraveNewDeal2019]
[^1]: On a personal note, I took advantage of this in buying myself a new car (a Honda Clarity).

What if we viewed questionable research practices (QRPs) as a collective action problem? Indeed, this has been discussed [elsewhere](https://www.chronicle.com/article/How-to-Fix-Psychology-s/233857)). Questionable research practices, which have been defined as "design, analytic, or reporting practices that have been questioned because of the potential for the practice to be employed with the purpose of presenting biased evidence in favor of an assertion" [@banksEditorialEvidenceQuestionable2016], are prevalent in applied psychology. Examples include selectively reporting results that are statistically significant, p-hacking, and adding and removing cases and variables to make results favorable to a particular point of view. Research by Banks et al. suggest that upward of 91% of studies contain evidence of questionable research practices. Scholars noted that QRPs are required to publish in top-tier outlets [@banksQuestionsQuestionableResearch2016]. Such practices pollute our science and our practice. 

In I/O psych (indeed, the organizational sciences at large), academics hope to generate work that helps better understand what makes organizational life the way it is, the broader aim of which is that, by understanding organizational life, we might make it better. To borrow and modify a phrase from Adam Grant, you might say we, by studying how work sucks, we might make it not suck. Think of our work as a set of cars. The published literature is filled with several automakers (i.e., researchers, research teams, universities) selling various "cars" (i.e., models, hypothesized effects, etc.) that organizations can use if they so wish to do so. Will the cars work, you might wonder? How can you, as an organizational stakeholder, decide which cars are worth their pay? 

Ideally, you could use similar kinds of tests that are used to test new vehicles: take the car for a spin. Perhaps the best way to think of it is to have a field experiment of sorts. Take the model from the published literature and apply it in your context to see if it works as claimed.

### Insights from a recent high-profile failure to control pollution...which should make these flaws quite salient

Consider for the moment that, in the US at the very least, we have the Environmental Protection Agency (EPA), which has been granted the authority (via the Clean Air Act) to regulate the automotive industry. Specifically, vehicles must comply with certain emissions standards are they cannot be sold in the US. Historically, vehicles have been tested to ensure compliance with standards using a dynanometer (i.e., a treadmill for a car that also tests emissions level). 

More recently, the Volkswagen emissions scandal revealed several flaws with the EPA’s approach to regulating pollution. One of the largest automakers in the world, [Volkswagen](https://en.wikipedia.org/wiki/Volkswagen_emissions_scandal), managed to organize a conspiracy to defeat emissions testing in the US. The scandal has cost VW over [$30 billion](https://money.cnn.com/2017/09/29/investing/volkswagen-diesel-cost-30-billion/index.html), which is a fraction of the [$130 billion](https://www.statista.com/statistics/665397/costs-resulting-from-volkswagen-diesel-emission-scandal/) cost burden VW’s cheating had on society at large (not to mention the thousands of lives that came to a premature [end](http://news.mit.edu/2017/volkswagen-emissions-premature-deaths-europe-0303). At the center of the scandal was a highly sophisticated defeat device. Defeat devices essentially are software that has been calibrated to (in VW’s case) detect testing conditions and change the car’s emissions output to pass the test. As an illustration of one part of how this particular device worked, when the steering wheel was stationary (as was typical during testing), the software treated this as an indication that the car was probably undergoing testing. As a consequence, engine output was muffled and the corresponding emissions levels were reduced. This scandal, which is revealing some shady activity in the [automotive industry](https://www.theguardian.com/commentisfree/2016/apr/29/car-industry-banking-emissions-scandal-vw), has prompted discussions regarding how cars are tested to ensure that they are compliant with the law. 

The VW emissions scandal prompted the EPA to change the way they test vehicles for compliance. Rather than relying on dynanometer testing, which can be easily gamed, new cars (model year 2015) must be tested on the road in [real settings](https://www.roadandtrack.com/new-cars/car-technology/news/a27288/epa-road-testing-vehicle-emissions/) with equipment that gauges the efficiency and emissions output of the vehicle. Real world testing, in fact, is how the VW's conspiracy to defeat emissions testing in the US was brought to an [end](https://www.citylab.com/equity/2015/09/the-study-that-brought-down-volkswagen/407149/).

# Carrots make tasty sticks

We the people are a raw deal on the research front. Taxpayers essentially have to pay for research three times over: (i) once when the public funding is used to pay for the research, (ii) again when the public has to pay for access to the research, and (iii) again when universities have to pay for access to whole scholarly databases. All of this is to explain how academic publishing is one of the most profitable industries to be in.

Which helps to make a point: the government can take care of this mess. Of the many proposals that can be discussed, I'm particularly interested in incentives to institutions and researchers for making their work essentially "born open" [see @rouderWhatWhyHow2016]. Much like tax credits can motivate consumers to adopt [cleaner vehicles](http://www.govtech.com/fs/transportation/Research-Shows-Incentives-to-Purchase-Electric-Vehicles-Are-Working.html), the government can create incentive structures that scholars to engage in open scholarship.
